{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we learned the mathematical defintion of a gradient.  We saw that the gradient of a function was a combination of our partial derivatives with respect to each variable of that function.  We saw the direction of gradient descent was simply to move in the negative direction of the gradient.  So if the direction of ascent of a function is a move up and to the right, the descent is down and to the left. In this lesson we will apply gradient descent to our cost function to see how we can move towards a best fit regression line by changing variables of $m$ and $b$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing RSS as a multivariable function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about why gradient descent applies so well to a cost function.  Initially, we said that the cost of our function, meaning the difference between what our regression line predicted and the dataset, changed as we altered the y-intercept or the slope of the function.\n",
    "\n",
    "Remember that mathematically, when we say cost function, we use residual sum of squares where $ RSS = \\sum(guess - actual)^2 = \\sum(\\overline{y} - y)^2 = \\sum(mx + b - y)^2$, for all $x$ values, and where $mx + b $ represents our regression line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regression-scatter.png](./regression-scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we when we just changed one variable of our regression line, $m$ or $b$, our cost function looked like a curve.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "line",
         "name": "RSS with changes to y-intercept",
         "text": [],
         "x": [
          70,
          80,
          90,
          100,
          110,
          120,
          130,
          140
         ],
         "y": [
          10852,
          9690,
          9128,
          9166,
          9804,
          11042,
          12880,
          15318
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"9bc17dc0-7847-4873-91f3-6cde5b10f179\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9bc17dc0-7847-4873-91f3-6cde5b10f179\", [{\"x\": [70, 80, 90, 100, 110, 120, 130, 140], \"y\": [10852, 9690, 9128, 9166, 9804, 11042, 12880, 15318], \"mode\": \"line\", \"name\": \"RSS with changes to y-intercept\", \"text\": []}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9bc17dc0-7847-4873-91f3-6cde5b10f179\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9bc17dc0-7847-4873-91f3-6cde5b10f179\", [{\"x\": [70, 80, 90, 100, 110, 120, 130, 140], \"y\": [10852, 9690, 9128, 9166, 9804, 11042, 12880, 15318], \"mode\": \"line\", \"name\": \"RSS with changes to y-intercept\", \"text\": []}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from graph import m_b_trace, trace_values, plot\n",
    "init_notebook_mode(connected=True)\n",
    "b_values = list(range(70, 150, 10))\n",
    "rss = [10852, 9690, 9128, 9166, 9804, 11042, 12880, 15318]\n",
    "cost_curve_trace = trace_values(b_values, rss, mode=\"line\", name = 'RSS with changes to y-intercept')\n",
    "plot([cost_curve_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in two dimensions, we simply had to move forwards or backwards by changing our, in this case y-intercept.  So the cost curve above indicates that changing the regression line from having a y-intercept of 70 to 80 would decrease our cost.\n",
    "\n",
    "Allowing us to change both variables, $m$ and $b$ means calculating how RSS varies with both $m$ and $b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So because our RSS, is a function of how we change our values of $m$ and $b$, statisticians express this mathematically by saying the cost function is the following:  \n",
    "$J(m, b) = \\sum(mx + b - y)^2$\n",
    "\n",
    "Let's just quickly translate this into what we know about RSS.  In the first part $J(m, b)$, $J$ just represents the residual sum of squares, which varies as the $m$ and $b$ variables of our regession line are changed.  After the equals sign, the variables $x$ and $y$ represent an actual point $x$, $y$ of our dataset.  So above this would be our existing data of a movie's budget and revenue.\n",
    "\n",
    "Our formula takes the difference between $mx + b$, the $y$ value our regression line predicts, and our actual $y$.  Then we square this difference, and sum up these squares for each piece of data in our dataset.  That is our formula above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just our other multivariable functions we have seen thus far, we can display it in three dimensions, and it looks like the following.\n",
    "\n",
    "![](./gradientdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the three dimensional graph below shows how the cost associated with our regression line changes as the slope and intercept values are changed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the gradient of our cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore how to use gradient descent to determine which direction to move changing both $m$ and $b$.  Applied to the general multivariable function $f(x,y)$, gradient descent answered how much move the $x$ variable and the $y$ variable to produce the greatest decrease in output.  Now that we are applying gradient descent to our cost curve $J(m, b)$, the technique should answer how much to move the $m$ variable and the $b$ variable to produce the greatest decrease in cost, or RSS. So when altering our regression line, how much of that change should come through changing the slope versus how much of that change should come from a change to the y-intercept.\n",
    "\n",
    "As we know, the gradient of a function is simply the partial derivatives with respect to each of the variables, so:\n",
    "\n",
    "$$ \\nabla J(m, b) = \\frac{\\delta J}{\\delta m}, \\frac{\\delta J}{\\delta b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In calculating the partial derivatives of our function $J(m,b) =  \\sum(mx + b - y)^2$, we won't change the result if we ignore the summation then replace it back at the end, so that's what we'll do to make our lives easier.\n",
    "\n",
    "Ok, so let's take our partial derivatives.   $$\\frac{\\delta J}{\\delta m}J(m, b) = \\frac{dJ}{dm}(mx + b - y)^2$$\n",
    "\n",
    "$$\\frac{\\delta J}{\\delta m}J(m, b) = \\frac{\\delta J}{\\delta b}(mx + b - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking our first partial derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with taking the partial derivative with respect to $m$.\n",
    "\n",
    "$$\\frac{\\delta J}{\\delta m}J(m, b) = \\frac{dJ}{dm}(mx + b - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is a tricky function to take the derivative of.  So we can use functional composition followed by the chain rule to make it easier.  Using functional composition, we can rewrite our function $J$ as two functions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$g(m,b) = mx +b - y$$\n",
    "\n",
    "$$J(g(m,b)) = (g(m,b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the chain rule to find the partial derivative with respect to a change in the slope, gives us:\n",
    "\n",
    "$$\\frac{dJ}{dm}J(g) = \\frac{dJ}{dg}J(g(m, b))*\\frac{dg}{dm}g(m,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to solve these derivatives individually: \n",
    "\n",
    "$$\\frac{dJ}{dg}J(g(m, b)) = \\frac{dJ}{dg}g(m,b)^2 = 2*g(m,b)$$\n",
    "\n",
    "$$\\frac{dg}{dm}g(m,b) =  \\frac{dg}{dm} (mx +b - y) = \\frac{dg}{dm}mx +\\frac{dg}{dm}b - \\frac{dg}{dm}y = x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plugging these back into our chain rule we have: \n",
    "\n",
    " $\\frac{dJ}{dg}J(g(m,b))*\\frac{dg}{dm}g(m,b) = (2*g(m,b))*x = 2*(mx + b -y)*x $\n",
    " \n",
    " So\n",
    " \n",
    "$$\\frac{\\delta J}{\\delta m}J(m, b) =  2*(mx + b -y)*x  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our second partial derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's calculate the partial derivative with respect to a change in the y-intercept.  We express this mathematically with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta J}{\\delta m}J(m, b) = \\frac{dJ}{dm}(mx + b - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then once again, we use functional composition following by the chain rule.  So we view our cost function as the same two functions $g(m,b)$ and $J(g(m,b))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g(m,b) = mx +b - y$$\n",
    "\n",
    "$$J(g(m,b)) = (g(m,b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So applying the chain rule, to this same function composition, we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dJ}{db}J(g) = \\frac{dJ}{dg}J(g)*\\frac{dg}{db}g(m,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our next step is to calculate these partial derivatives individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our earlier calculation of the partial derivative, we know that $\\frac{dJ}{dg}J(g(m,b)) = \\frac{dJ}{dg}g(m,b)^2 = 2*g(m,b)$.  The only thing left to calculate is $\\frac{dg}{db}g(m,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{dg}{db}g(m,b) = \\frac{dg}{db}(mx + b - y) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plug our terms into our chain rule and get: \n",
    "\n",
    "$$ \\frac{dJ}{dg}J(g)*\\frac{dg}{db}g(m,b) = 2*g(m,b)*1 = 2*(mx + b -y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our two partial derivatives for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have our two partial derivatives.  But as we know, to move point us in the direction of greatest descent we have to reverse the direction of each of these derivatives by reversing the sign, giving us:\n",
    "\n",
    "* $ \\frac{dJ}{dm}J(m,b) = -2*x(mx + b -y) $\n",
    "* $ \\frac{dJ}{db}J(m,b) = -2*(mx + b -y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as $mx + b$ = is just our regression line, we can simplify these formulas to be: \n",
    "\n",
    "* $ \\frac{dJ}{dm}J(m,b) = -2*x(\\overline{y} - y) $\n",
    "* $ \\frac{dJ}{db}J(m,b) = -2*(\\overline{y} - y) $\n",
    "\n",
    "and adding back in our summations we have: \n",
    "\n",
    "* $ \\frac{dJ}{dm}J(m,b) = -2*\\sum x(\\overline{y} - y) $\n",
    "* $ \\frac{dJ}{db}J(m,b) = -2*\\sum(\\overline{y} - y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is what what we'll do to find the our \"best fit regression line.\"  We'll start with an initial regression line with values of $m$ and $b$.  Then we'll go through our dataset, and with each point will use the above formulas to tell us how to update our regression line such that it continues to minimize our cost function.  We'll spend the next lesson we'll walking through this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section we developed some intuition for why the gradient of a function is the direction of steepest ascent and the negative gradient of a function is the direction of steepest decent.  Essentially, the gradient uses the partial derivatives to see what change will result from a of any of the function's dimensions, and then moves in that direction weights towards the partial derivative with the larger magnitude.\n",
    "\n",
    "We also practiced calculating some gradients, and ultimately calculated the gradient for our cost function.  This gave us two formulas which tell us how to update our regression line so that it descends along our cost function and approaches a \"best fit line\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
